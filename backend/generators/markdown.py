"""
Markdown ç”Ÿæˆå™¨æ¨¡å—
"""
from datetime import datetime, timedelta
from typing import List, Dict
from pathlib import Path
import glob
import os
import re
from backend.config import Config


def _fix_github_url(url: str, description: str = '') -> str:
    """
    ä¿®å¤GitHub Trendingè¿”å›çš„é”™è¯¯é“¾æ¥

    GitHub Trendingçš„RSSå¯èƒ½è¿”å›ç™»å½•é¡µé¢æˆ–èµåŠ©é¡µé¢é“¾æ¥ï¼Œ
    éœ€è¦ä»æè¿°ä¸­æå–æ­£ç¡®çš„ä»“åº“é“¾æ¥

    Args:
        url: åŸå§‹URL
        description: æ–‡ç« æè¿°ï¼Œå¯èƒ½åŒ…å«æ­£ç¡®çš„é“¾æ¥

    Returns:
        ä¿®å¤åçš„æ­£ç¡®URL
    """
    # å¦‚æœURLä¸ºç©ºï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
    if not url:
        return ''

    # å¦‚æœURLå·²ç»æ˜¯æ­£ç¡®çš„GitHubä»“åº“é“¾æ¥ï¼Œç›´æ¥è¿”å›
    if re.match(r'^https://github\.com/[^/]+/[^/]+/?$', url):
        return url

    # å¦‚æœæ˜¯ç™»å½•é¡µé¢æˆ–èµåŠ©é¡µé¢ï¼Œå°è¯•ä»æè¿°ä¸­æå–
    if '/login?' in url or '/sponsors' in url or '/account' in url:
        # å°è¯•ä»æè¿°ä¸­æå–æ­£ç¡®çš„GitHubä»“åº“é“¾æ¥
        if description:
            # åŒ¹é… GitHub ä»“åº“é“¾æ¥æ¨¡å¼
            match = re.search(r'https://github\.com/[^/]+/[^/]+', description)
            if match:
                return match.group(0).rstrip('/')

    # å¦‚æœURLåŒ…å«return_toå‚æ•°ï¼Œä»ä¸­æå–
    if 'return_to=' in url:
        match = re.search(r'return_to=%2F([^%]+)%2F([^%]+)', url)
        if match:
            owner = match.group(1)
            repo = match.group(2)
            return f"https://github.com/{owner}/{repo}"

    # è¿”å›åŸå§‹URL
    return url


def _normalize_url(url: str, base_url: str = '') -> str:
    """
    æ ‡å‡†åŒ–URLï¼Œç¡®ä¿æ˜¯å¯ç‚¹å‡»çš„ç»å¯¹é“¾æ¥

    Args:
        url: åŸå§‹URL
        base_url: åŸºç¡€URLï¼ˆç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰

    Returns:
        æ ‡å‡†åŒ–åçš„URL
    """
    if not url:
        return ''

    url = url.strip()

    # å¦‚æœæ˜¯ç»å¯¹URLï¼Œç›´æ¥è¿”å›
    if url.startswith('http://') or url.startswith('https://'):
        return url

    # å¦‚æœæ˜¯ç›¸å¯¹URLï¼Œå°è¯•åŸºäºbase_urlæ„å»ºç»å¯¹URL
    if url.startswith('/') and base_url:
        from urllib.parse import urlparse
        parsed = urlparse(base_url)
        return f"{parsed.scheme}://{parsed.netloc}{url}"

    # å…¶ä»–æƒ…å†µè¿”å›åŸå§‹URL
    return url


def generate_markdown(articles: List[Dict], output_path: str = None) -> str:
    """
    ç”Ÿæˆ Markdown æ ¼å¼çš„æ–‡ç« åˆ—è¡¨

    Args:
        articles: æ–‡ç« åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

    Returns:
        Markdown å†…å®¹å­—ç¬¦ä¸²
    """
    if not articles:
        return "# ä»Šæ—¥æ–°é—»\n\næš‚æ— æ–‡ç« \n"

    # æŒ‰å‘å¸ƒæ—¶é—´å€’åº
    sorted_articles = sorted(articles, key=lambda x: x.get('published_at', ''), reverse=True)

    # æŒ‰æ¥æºåˆ†ç»„
    sources = {}
    for article in sorted_articles:
        source_name = article.get('source_name', 'Unknown')
        if source_name not in sources:
            sources[source_name] = []
        sources[source_name].append(article)

    # ç”Ÿæˆ Markdown
    date_str = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    lines = [
        f"# ä»Šæ—¥æ–°é—» - {date_str}",
        "",
        f"æ€»è®¡: {len(articles)} ç¯‡",
        "",
        "---",
        ""
    ]

    for source_name, source_articles in sources.items():
        lines.append(f"## {source_name}")
        lines.append("")
        for article in source_articles:
            title = article.get('title', 'æ— æ ‡é¢˜')
            url = article.get('url', '')
            source_url = article.get('source_url', '')  # ç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„
            published_at = article.get('published_at', '')
            translated_title = article.get('translated_title')
            description = article.get('description', '') or article.get('content', '')[:200]

            # æ ¼å¼åŒ–æ—¥æœŸ
            if published_at:
                try:
                    dt = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                    date_str = dt.strftime('%mæœˆ%dæ—¥ %H:%M')
                except:
                    date_str = published_at[:10] if len(published_at) >= 10 else ''
            else:
                date_str = ''

            # æ ‡é¢˜è¡Œ
            if translated_title:
                lines.append(f"### {title}")
                lines.append(f"*{translated_title}*")
            else:
                lines.append(f"### {title}")

            if date_str:
                lines.append(f"> ğŸ“… {date_str}")

            if url:
                # å…ˆä¿®å¤GitHub Trendingçš„ç™»å½•é¡µé¢é“¾æ¥
                fixed_url = _fix_github_url(url, description)
                # å†æ ‡å‡†åŒ–æ‰€æœ‰URL
                normalized_url = _normalize_url(fixed_url, source_url)
                if normalized_url:
                    lines.append(f"> [é˜…è¯»åŸæ–‡]({normalized_url})")

            if description:
                lines.append(f"> {description}...")

            lines.append("")

        lines.append("---")
        lines.append("")

    lines.append(f"*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*")

    markdown_content = '\n'.join(lines)

    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œå†™å…¥æ–‡ä»¶
    if output_path:
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)

    return markdown_content


def generate_daily_markdown(articles: List[Dict], date: datetime = None) -> str:
    """
    ç”Ÿæˆæ¯æ—¥æ–°é—» Markdown æ–‡ä»¶ï¼ˆå¸¦æ—¶é—´æˆ³ï¼Œä¿ç•™æ¯æ¬¡æŠ“å–è®°å½•ï¼‰

    Args:
        articles: æ–‡ç« åˆ—è¡¨
        date: æ—¥æœŸæ—¶é—´ï¼ˆé»˜è®¤ä¸ºå½“å‰æ—¶é—´ï¼‰

    Returns:
        è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    if date is None:
        date = datetime.now()

    # ç”Ÿæˆæ–‡ä»¶åï¼šnews_YYYY-MM-DD_HH-MM.md
    date_str = date.strftime('%Y-%m-%d')
    time_str = date.strftime('%H-%M')
    filename = f"news_{date_str}_{time_str}.md"
    output_path = Config.DATA_DIR / filename

    # ç”Ÿæˆå†…å®¹
    content = generate_markdown(articles, str(output_path))

    return str(output_path)


def generate_latest_markdown(articles: List[Dict]) -> str:
    """
    ç”Ÿæˆæœ€æ–°æ–°é—» Markdown æ–‡ä»¶ï¼ˆlatest.mdï¼‰

    Args:
        articles: æ–‡ç« åˆ—è¡¨

    Returns:
        è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    output_path = Config.DATA_DIR / "latest.md"
    content = generate_markdown(articles, str(output_path))
    return str(output_path)


def cleanup_old_markdown_files(days: int = 30) -> int:
    """
    æ¸…ç†æ—§çš„ Markdown æ–‡ä»¶ï¼Œä¿ç•™æœ€è¿‘ N å¤©

    Args:
        days: ä¿ç•™å¤©æ•°ï¼Œé»˜è®¤ä¸º 30 å¤©

    Returns:
        åˆ é™¤çš„æ–‡ä»¶æ•°é‡
    """
    cutoff_date = datetime.now() - timedelta(days=days)
    deleted_count = 0

    # åŒ¹é…æ‰€æœ‰ news_YYYY-MM-DD_HH-MM.md æ–‡ä»¶
    pattern = str(Config.DATA_DIR / "news_*.md")
    files = glob.glob(pattern)

    for file_path in files:
        try:
            # ä»æ–‡ä»¶åæå–æ—¥æœŸæ—¶é—´
            filename = os.path.basename(file_path)
            # æ ¼å¼: news_YYYY-MM-DD_HH-MM.md
            date_str = filename[5:-3]  # å»æ‰ "news_" å’Œ ".md"

            if len(date_str) == 15:  # YYYY-MM-DD_HH-MM
                file_date = datetime.strptime(date_str, '%Y-%m-%d_%H-%M')

                if file_date < cutoff_date:
                    os.remove(file_path)
                    deleted_count += 1
                    print(f"Deleted old file: {filename}")
        except Exception as e:
            print(f"Error processing file {file_path}: {e}")

    if deleted_count > 0:
        print(f"Cleaned up {deleted_count} old markdown files")
    else:
        print("No old markdown files to clean up")

    return deleted_count
